from __future__ import division
from __future__ import unicode_literals
from __future__ import absolute_import

import collections
import io
import os
import zipfile
import urllib2

import h5py
import numpy as np
import marisa_trie
from gensim.models import word2vec

from nlp_data.base import get_data_home
from nlp_data.parallel import get_n_jobs
from nlp_data.nlp_io.progress_bar import chunk_read, chunk_report
from nlp_data.word_vectors.enums import WordVectorTypes
from nlp_data.word_vectors import glove_reader


URL = "http://nlp.stanford.edu/data/glove.6B.zip"
ARCHIVE_NAME = "glove.6B.zip"
GLOVE_FOLDER = "glove.6B"


def check_vocab(vocabulary):
    if len(vocabulary) == 0:
        raise ValueError('Vocabulary is empty')

    return vocabulary


def from_stanford(filename, dtype=np.float64, freeze_vocabulary=True, n_jobs=-1):
    """
    Load model from the output files generated by
    the C code from http://nlp.stanford.edu/projects/glove/.

    The entries of the word dictionary will of type
    unicode in Python 2 and str in Python 3.
    """
    n_jobs = get_n_jobs(n_jobs)
    vocab = glove_reader.from_stanford(filename, n_jobs=n_jobs)
    vocab = check_vocab(vocab)

    if n_jobs == 1:
        vocabulary = {}
        vectors = array.array('f' if dtype == np.float32 else 'd')

        with io.open(filename, 'r', encoding='utf-8') as savefile:
            # read this in multiple processes. I hate waiting...
            for i, line in enumerate(savefile):
                tokens = line.strip().split(' ')

                word = tokens[0]
                entries = tokens[1:]

                vocabulary[word] = i
                vectors.extend(dtype(x) for x in entries)

        # Infer word vectors dimension
        no_components = len(entries)
        no_vectors = len(vocabulary)
        vectors = np.array(vectors, dtype=dtype).reshape(no_vectors, no_components)

    else:
        if freeze_vocabulary:  # use a marisa-trie storage
            vocabulary = marisa_trie.Trie(vocab.keys(), order=marisa_trie.LABEL_ORDER)
            vectors = np.zeros((len(vocabulary), int(next(vocab.itervalues()).shape[0])), dtype=dtype)
            for word, vector in vocab.iteritems():
                idx = vocabulary[word]
                vectors[idx, :] = vector
        else:
            vocabulary = collections.OrderedDict()
            for idx, word in enumerate(vocab):
                vocabulary[word] = idx
            vectors = np.vstack(vocab.itervalues()).astype(dtype)

    return vectors, vocabulary


def to_hdf5(w2v_filename,
            hdf5_filename,
            w2v_type,
            dtype=np.float32,
            n_jobs=-1):

    if w2v_type != WordVectorTypes.GLOVE:
        raise ValueError("w2v_type not recognized.")

    vectors, vocabulary = from_stanford(
            w2v_filename, dtype=dtype, n_jobs=n_jobs)

    with h5py.File(hdf5_filename, 'w') as h5file:
        vector_shape = vectors.shape
        vector_dtype = vectors.dtype
        dset = h5file.create_dataset(
                'embeddings',
                shape=vector_shape,
                dtype=vector_dtype,
                compression='gzip',
                compression_opts=9)
        dset[:] = vectors
        dset.attrs['shape'] = vector_shape

        vocabulary = np.array(vocabulary.keys())
        dt = h5py.special_dtype(vlen=unicode)
        dset = h5file.create_dataset(
                'vocabulary',
                shape=vocabulary.shape,
                dtype=dt)
        dset[:] = vocabulary


def from_hdf5(path, keys=None):
    with h5py.File(path, 'r') as h5file:
        vocab = h5file['vocabulary'][:]
        vocabulary = collections.OrderedDict()
        if keys is None:
            for idx, word in enumerate(vocab):
                vocabulary[word] = idx + 1
            embeddings = h5file['embeddings'][:]
        else:
            valid_ids = np.where(np.in1d(vocab, keys))[0]
            for new_id, old_id in enumerate(valid_ids):
                vocabulary[vocab[old_id]] = new_id + 1
            embeddings = h5file['embeddings'][valid_ids, :]

        return embeddings, vocabulary


def _hdf5_filename(glove_filename):
    return glove_filename.rsplit('.', 1)[0] + '.hdf5'


def _listdir(path, full_path=True):
    if full_path:
        return [os.path.join(path, f) for f in os.listdir(path)]
    return os.listdir(path)


def files_to_hdf5(container_path):
    for f in _listdir(container_path, full_path=True):
        extract = _hdf5_filename(f)
        print('Converting {} -> {}'.format(f, extract))
        to_hdf5(f, extract, w2v_type=WordVectorTypes.GLOVE)


def download_glove(target_dir):
    archive_path = os.path.join(target_dir, ARCHIVE_NAME)

    if not os.path.exists(target_dir):
        os.makedirs(target_dir)

    print(80 * '_')
    print("Downloading glove word vectors to {}".format(archive_path))
    opener = urllib2.urlopen(URL)
    with open(archive_path, 'wb') as f:
        f.write(chunk_read(opener, report_hook=chunk_report))

    print(80 * '_')
    print("Extrating glove word vectors to {}".format(target_dir))
    with zipfile.ZipFile(archive_path) as zfile:
        zfile.extractall(path=target_dir)
    os.remove(archive_path)

    files_to_hdf5(target_dir)


def _glove_filename(glove_type, dimension):
    data_home = get_data_home()
    glove_file = '.'.join(["glove", glove_type, str(dimension) + 'd', 'hdf5'])
    glove_file = os.path.join(data_home, 'glove_home', GLOVE_FOLDER, glove_file)
    return glove_file


def to_gensim(embeddings, vocabulary):
    model = word2vec.Word2Vec()
    model.syn0 = embeddings

    vocab_size = len(vocabulary)
    for word, word_id in vocabulary.iteritems():
        model.vocab[word] = word2vec.Vocab(index=word_id, count=vocab_size - word_id)
        model.index2word.append(word)

    return model


def fetch_glove(n_vocab="6B", n_dimensions=100, use_gensim=False):
    data_home = get_data_home()
    cache_path = os.path.join(data_home, 'glove_home', GLOVE_FOLDER)
    glove_home = os.path.join(data_home, 'glove_home', GLOVE_FOLDER)
    glove_file = _glove_filename(n_vocab, n_dimensions)
    cache = None

    if os.path.exists(cache_path):
        try:
            if os.path.exists(glove_file):
                cache = glove_file
        except Exception as e:
            print(80 * '_')
            print('Cache loading failed')
            print(80 * '_')
            print(e)
    if cache is None:
        download_glove(target_dir=glove_home)
        cache = glove_file

    embeddings, vocab = from_hdf5(cache)

    if use_gensim:
        return to_gensim(embeddings, vocab)
    return embeddings, vocab
